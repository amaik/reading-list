# Reading List
My personal reading list.

## Transformer
### Transformer
- [x] Paper - Attention is all you need - [link](https://arxiv.org/abs/1706.03762)
- [x] Paper - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - [link](https://arxiv.org/abs/1810.04805)
- [ ] Paper - RoBERTa: A Robustly Optimized BERT Pretraining Approach - [link](https://arxiv.org/abs/1907.11692)
- [x] Paper - ALBERT: A Lite BERT for Self-supervised Learning of Language Representations - [link](https://arxiv.org/abs/1909.11942)
- [ ] Paper - Music Transformer - [link](https://arxiv.org/pdf/1809.04281.pdf)
- [x] Paper - TransformerXL - [link](https://arxiv.org/abs/1901.02860)
- [ ] Paper - XLNet - [link](https://arxiv.org/abs/1906.08237)
- [ ] Blog - The annotated transformer - [link](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [x] Blog - The illustrated transformer - [link](https://jalammar.github.io/illustrated-transformer/)
- [x] Blog - Visualized Bert/Elmo - [link](http://jalammar.github.io/illustrated-bert/)
- [ ] Blog - BERT First time - [link](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
- [ ] Notebook - Bert end-to-end - fine tuning - [link](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=7wzwke0sxS6W)


### GPT2/3
- [ ] Paper - GPT1 - [link](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [x] Paper - GPT2 - [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [ ] Paper - GPT3 - [link](https://arxiv.org/abs/2005.14165)
- [x] Blog - GPT1 - [link](https://openai.com/blog/language-unsupervised/)
- [x] Blog - GPT2 - [link](https://openai.com/blog/better-language-models/)
- [x] Blog - The illustrated GPT2 - [link](http://jalammar.github.io/illustrated-gpt2/)

## LM General
- [ ] Paper - Allow LMs to fill in the gap - [link](https://arxiv.org/abs/2005.05339)
- [ ] Paper - Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism - [link](https://arxiv.org/abs/1909.08053)

## Corpora
- [ ] Paper - BooksCorpus - [link](https://arxiv.org/abs/1506.06724)

## Tokenization
### Subword modelling
- [ ] Blog - Various algorithms - [link](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)
- [x] Paper - Byte-pair representation - [link](https://arxiv.org/abs/1508.07909  )
- [ ] **Paper - Word-piece representation - [link](https://arxiv.org/abs/1609.08144)**
- [x] Paper - SentencePiece: A simple and language independent subword tokenizer - [link](https://arxiv.org/abs/1808.06226)

## Representation
- [ ] Paper - ConveRT: Efficient and Accurate Conversational Representations from Transformers - [link](https://arxiv.org/abs/1911.03688)
- [ ] Paper - Glove - [link](https://nlp.stanford.edu/pubs/glove.pdf)


## Machine Learning - General
- [ ] Paper - Convolutional s2s models(learned positional embeddings) - [link](https://arxiv.org/abs/1705.03122)
- [ ] Paper - Layer normalization - [link](https://arxiv.org/abs/1607.06450)
- [ ] Paper - Batch normalization - [link](https://arxiv.org/abs/1502.03167)
- [x] Paper - Adam: A Method for Stochastic Optimization - [link](https://arxiv.org/abs/1412.6980)
- [x] Paper - Dropout - [link](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
- [ ] Paper - Label smoothing - [link](https://arxiv.org/abs/1512.00567)
- [ ] Paper - One model to learn them all (Task conditioning) - [link](https://arxiv.org/abs/1706.05137)
- [ ] Paper - Shannon Entropy - [link](https://arxiv.org/ftp/arxiv/papers/1405/1405.2061.pdf)
- [ ] Paper - The Reversible Residual Network: Backpropagation Without Storing Activations - [link](https://arxiv.org/abs/1707.04585)
- [ ] Paper - Weight Normalization - [link](https://arxiv.org/abs/1602.07868)
- [ ] Paper - TabNet - [link](https://arxiv.org/abs/1908.07442)
- [ ] Paper - Lookahead - [link](https://arxiv.org/abs/1907.08610v1)
- [ ] Paper - Knowledge Graph Embeddings and Explainable AI - [link](https://arxiv.org/abs/2004.14843)
- [ ] Paper - TreeLSTM - [link](https://arxiv.org/pdf/1503.00075.pdf)

## Graph Neural Networks
- [] Paper - Neural Message Passing for Quantum Chemistry - [link](https://arxiv.org/abs/1704.01212)


## Topic Modelling
- [ ] Paper - Topic modelling with Wasserstein Autoencoders - [link](https://arxiv.org/abs/1907.12374)

## Text Summarization
- [ ] Paper - Sample Efficient Text Summarization Using a Single Pre-Trained Transformer - [link](https://arxiv.org/abs/1905.08836)


## Misc
- [x] Website - Summary of papers with code. Leaderboard of benchmarks. - [link](https://paperswithcode.com/task/language-modelling)
- [ ] Paper - Conditional Random Fields - [link](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)
- [ ] Paper - Node2Vec (Representational Learning for graphs) [link](https://arxiv.org/abs/1607.00653)
